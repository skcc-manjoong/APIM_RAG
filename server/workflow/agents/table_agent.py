import traceback
import pandas as pd
import json
from utils.prompts import build_table_summary_messages, table_prompt_meta

class TableAgent:
    def __init__(self, llm):
        self.llm = llm
        self.role = "tableagent"
        
    async def run(self, state=None, cloud_result=None, user_request=None):
        # state ê°ì²´ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì—†ìœ¼ë©´ ì¸ìê°’ë§Œ ì‚¬ìš©
        if state:
            cloud_result = state.get("cloud_result")
            rag_result = state.get("rag_result") or []
            interactive_result = state.get("interactive_result") or {}
            user_request = next((m["content"] for m in reversed(state["messages"]) if m["role"] == "user"), "")
            # ì¸í„°ë™ì…˜ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„  ìš”ì•½í•œë‹¤
            if interactive_result:
                visit_trace = interactive_result.get("visit_trace") or []
                final_dom = interactive_result.get("final_dom") or ""
                # ì»¨í…ìŠ¤íŠ¸: ë°©ë¬¸ ê²½ë¡œ + ìµœì¢… DOM
                lines = ["[ë°©ë¬¸ ê²½ë¡œ]"]
                for item in visit_trace:
                    url = item.get("url", "")
                    path = item.get("path", "")
                    decision = item.get("decision")
                    if decision:
                        lines.append(f"- url={url} path={path} decision={json.dumps(decision, ensure_ascii=False)}")
                    else:
                        lines.append(f"- url={url} path={path}")
                lines.append("\n[ìµœì¢… DOM ìš”ì•½]")
                lines.append(final_dom[:2000])
                context = "\n".join(lines)
                # ê·¼ê±°: ë°©ë¬¸í•œ URL/pathë§Œ ì¶”ì¶œ
                evidence_lines = []
                for item in visit_trace:
                    evidence_lines.append(f"- URL: {item.get('url','')} | path: {item.get('path','')}")
                evidence_block = "\n".join(evidence_lines)
                messages = build_table_summary_messages(user_request, context, evidence_block)
                try:
                    summary_response = await self.llm.ainvoke(messages)
                    summary = summary_response.content
                    if state:
                        header = "ğŸ§­ ì‹¤ì œ UI ë‹¨ê³„ë³„ ìš”ì•½/ê²½ë¡œ í‘œ"
                        final_response = f"{header}\n\n{summary}"
                        state["messages"].append({"role": self.role, "content": final_response})
                        return {**state, "response": final_response}
                    else:
                        return {"summary": summary}
                except Exception as e:
                    error_msg = f"ìš”ì•½ ì‹¤íŒ¨: {str(e)}"
                    traceback.print_exc()
                    if state:
                        state["messages"].append({"role": "error", "content": error_msg})
                        return {**state, "response": error_msg}
                    else:
                        return {"summary": error_msg}
            
            # ê¸°ì¡´ ëª¨ë“œ: RAG ìš”ì•½
            if not cloud_result and not rag_result:
                error_msg = "APIM íƒìƒ‰ ê²°ê³¼ë‚˜ RAG ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"
                state["messages"].append({"role": "error", "content": error_msg})
                return {**state, "response": f"ìš”ì•½ ì‹¤íŒ¨: {error_msg}"}
            
        # 1. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±: rag_result ìƒìœ„ ì²­í¬ë§Œ ì¶”ì¶œ + ê·¼ê±° ì¤€ë¹„
        evidence_lines = []
        if not cloud_result:
            top_k = 5
            chunks = []
            for i, r in enumerate(rag_result[:top_k], 1):
                doc = r.get("document", {})
                name = doc.get("name", f"chunk_{i}")
                snippet = (doc.get("search_text", "") or "").strip().replace("\n", " ")[:200]
                sim = r.get("similarity")
                evidence_lines.append(f"- {name} (sim={sim:.2f}): {snippet}")
                chunks.append(doc.get("search_text", ""))
            context = "\n\n---\n\n".join(chunks)
        else:
            context = cloud_result

        # 2. LLM ìš”ì•½ (ì—­í• /ê·œì¹™/í˜•ì‹ í…œí”Œë¦¿)
        evidence_block = "\n".join(evidence_lines)
        messages = build_table_summary_messages(user_request, context, evidence_block)
        try:
            summary_response = await self.llm.ainvoke(messages)
            summary = summary_response.content
            
            if state:
                header = "ğŸ“š APIM Document ê¸°ë°˜ ìš”ì•½/í‘œ"
                final_response = f"{header}\n\n{summary}"
                state["messages"].append({"role": self.role, "content": final_response})
                return {**state, "response": final_response}
            else:
                return {"summary": summary}
                
        except Exception as e:
            error_msg = f"ìš”ì•½ ì‹¤íŒ¨: {str(e)}"
            traceback.print_exc()
            
            if state:
                state["messages"].append({"role": "error", "content": error_msg})
                return {**state, "response": error_msg}
            else:
                return {"summary": error_msg} 