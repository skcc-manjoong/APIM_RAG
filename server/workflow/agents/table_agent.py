import traceback
import pandas as pd
import json

class TableAgent:
    def __init__(self, llm):
        self.llm = llm
        self.role = "tableagent"
        
    async def run(self, state=None, cloud_result=None, user_request=None):
        # state ê°ì²´ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì—†ìœ¼ë©´ ì¸ìê°’ë§Œ ì‚¬ìš©
        if state:
            cloud_result = state.get("cloud_result")
            rag_result = state.get("rag_result") or []
            screenshot_result = state.get("screenshot_result")
            user_request = next((m["content"] for m in reversed(state["messages"]) if m["role"] == "user"), "")
            if not cloud_result and not rag_result:
                error_msg = "í´ë¼ìš°ë“œ ê²°ê³¼ë‚˜ RAG ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"
                state["messages"].append({"role": "error", "content": error_msg})
                return {**state, "response": f"ìš”ì•½ ì‹¤íŒ¨: {error_msg}"}
                
        # 1. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±: rag_result ìƒìœ„ ì²­í¬ë§Œ ì¶”ì¶œ + ê·¼ê±° ì¤€ë¹„
        evidence_lines = []
        if not cloud_result:
            top_k = 5
            chunks = []
            for i, r in enumerate(rag_result[:top_k], 1):
                doc = r.get("document", {})
                name = doc.get("name", f"chunk_{i}")
                snippet = (doc.get("search_text", "") or "").strip().replace("\n", " ")[:200]
                sim = r.get("similarity")
                evidence_lines.append(f"- {name} (sim={sim:.2f}): {snippet}")
                chunks.append(doc.get("search_text", ""))
            context = "\n\n---\n\n".join(chunks)
        else:
            context = cloud_result

        # 2. LLM ìš”ì•½ (ì»¨í…ìŠ¤íŠ¸ ì™¸ ì¶”ë¡  ê¸ˆì§€ + í‘œ ê°•ì œ)
        evidence_block = "\n".join(evidence_lines)
        summary_prompt = f"""
ì—­í• : ë„ˆëŠ” APIM ê´€ë¦¬ìë‹¤. ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ê·¼ê±°ë¡œ ê°„ê²°í•œ ì„¤ëª…ê³¼ í‘œ(ë§ˆí¬ë‹¤ìš´)ë¥¼ ìƒì„±í•˜ë¼.
ê·œì¹™:
- ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì‚¬ì‹¤ì€ ì“°ì§€ ë§ ê²ƒ(ì¶”ë¡ /ìƒì‹ ê¸ˆì§€)
- í‘œëŠ” í—¤ë” í¬í•¨, ìµœëŒ€ 10í–‰
- ë‹µë³€ ë§ˆì§€ë§‰ì— 'ê·¼ê±°' ì„¹ì…˜ìœ¼ë¡œ ì‚¬ìš©í•œ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ í¬í•¨í•  ê²ƒ

[ì‚¬ìš©ì ìš”ì²­]
{user_request}

[ì»¨í…ìŠ¤íŠ¸]
{context}

[ê·¼ê±°]
{evidence_block}
"""
        try:
            summary_response = await self.llm.ainvoke([{"role": "user", "content": summary_prompt}])
            summary = summary_response.content
            
            # state ê°ì²´ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ summaryë§Œ ë°˜í™˜
            if state:
                # ìŠ¤í¬ë¦°ìƒ· ì •ë³´ê°€ ìˆìœ¼ë©´ ì‘ë‹µì— í¬í•¨
                final_response = f"ìš”ì•½í•œ ê²°ê³¼ì…ë‹ˆë‹¤.\n\n{summary}"
                if screenshot_result:
                    final_response += f"\n\nğŸ“¸ ê´€ë ¨ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ·: {screenshot_result.get('url', '')}"
                
                state["messages"].append({"role": self.role, "content": summary})
                return {**state, "response": final_response}
            else:
                return {"summary": summary}
                
        except Exception as e:
            error_msg = f"ìš”ì•½ ì‹¤íŒ¨: {str(e)}"
            traceback.print_exc()
            
            if state:
                state["messages"].append({"role": "error", "content": error_msg})
                return {**state, "response": error_msg}
            else:
                return {"summary": error_msg} 